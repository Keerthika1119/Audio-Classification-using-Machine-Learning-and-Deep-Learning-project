{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc82552c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, request, jsonify import numpy as np\n",
    "import librosa\n",
    "from keras.models import load_model\n",
    "from sklearn.preprocessing import LabelEncoder import os\n",
    "import joblib import logging\n",
    "app = Flask( name ) logging.basicConfig(level=logging.DEBUG) emotion_model = load_model('models/emotion_model.h5') genre_model = joblib.load('models/genre_model.pkl')\n",
    "emotion_labels = ['neutral', 'calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised'] genre_labels = ['blues', 'pop', 'rock', 'classical', 'jazz', 'metal', 'country', 'disco', 'hiphop', 'reggae']\n",
    "\n",
    "emotion_label_encoder = LabelEncoder() emotion_label_encoder.fit(emotion_labels)\n",
    "\n",
    "genre_label_encoder = LabelEncoder() genre_label_encoder.fit(genre_labels)\n",
    "\n",
    "logging.debug(f\"Genre labels: {genre_labels}\")\n",
    "logging.debug(f\"Encoded genre labels: {genre_label_encoder.transform(genre_labels)}\")\n",
    "\n",
    "\n",
    "def extract_features(file_path):\n",
    " \n",
    "try:\n",
    "audio, sr = librosa.load(file_path, sr=22050, duration=2.5, offset=0.6) mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)\n",
    "return np.mean(mfcc.T, axis=0) except Exception as e:\n",
    "logging.error(f\"Error extracting features from {file_path}: {e}\") raise\n",
    "\n",
    "@app.route('/') def index():\n",
    "return render_template('index.html')\n",
    "\n",
    "\n",
    "@app.route('/predict_emotion', methods=['POST']) def predict_emotion():\n",
    "try:\n",
    "file = request.files['audio']\n",
    "file_path = os.path.join('static', file.filename) file.save(file_path)\n",
    "\n",
    "features = extract_features(file_path) features = np.array([features])\n",
    "prediction = emotion_model.predict(features)\n",
    "predicted_emotion = emotion_label_encoder.inverse_transform([np.argmax(prediction)])\n",
    "\n",
    "\n",
    "return jsonify({\"emotion\": predicted_emotion[0]})\n",
    " \n",
    "logging.error(f\"Error in emotion prediction: {e}\") return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "@app.route('/predict_genre', methods=['POST']) def predict_genre():\n",
    "try:\n",
    "file = request.files['audio']\n",
    "file_path = os.path.join('static', file.filename) file.save(file_path)\n",
    "\n",
    "features = extract_features(file_path) features = np.array([features])\n",
    "\n",
    "logging.debug(f\"Extracted features: {features}\")\n",
    "\n",
    "\n",
    "prediction = genre_model.predict(features) logging.debug(f\"Raw prediction output: {prediction}\") if isinstance(prediction[0], np.ndarray:\n",
    "predicted_genre_index = np.argmax(prediction[0]) else:\n",
    "predicted_genre_index = prediction[0]\n",
    "predicted_genre = genre_label_encoder.inverse_transform([predicted_genre_index]) logging.debug(f\"Predicted genre: {predicted_genre[0]}\")\n",
    "\n",
    "return jsonify({\"genre\": predicted_genre[0]})\n",
    " \n",
    "logging.error(f\"Error in genre prediction: {e}\") return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "if  name\t== \" main \": app.run(debug=True)\n",
    "\n",
    "4.4. Step-4: Code in index.html:\n",
    "\n",
    "\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "<meta charset=\"UTF-8\">\n",
    "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "<title>Audio Emotion and Genre Prediction</title>\n",
    "<style>\n",
    "body {\n",
    "font-family: Arial, sans-serif; margin: 0;\n",
    "padding: 0;\n",
    "background-color: #f4f4f9; color: #333;\n",
    "}\n",
    "\n",
    "\n",
    "h1 {\n",
    "text-align: center; margin-top: 30px;\n",
    " \n",
    "color: #333;\n",
    "}\n",
    "\n",
    "\n",
    "h2 {\n",
    "color: #444;\n",
    "}\n",
    "\n",
    "\n",
    ".container { width: 80%;\n",
    "max-width: 800px; margin: 0 auto; padding: 20px; background-color: #fff;\n",
    "box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1); border-radius: 10px;\n",
    "margin-top: 40px;\n",
    "}\n",
    "\n",
    "\n",
    "form {\n",
    "display: flex;\n",
    "flex-direction: column; gap: 15px;\n",
    "margin-bottom: 30px;\n",
    "}\n",
    "\n",
    "\n",
    "input[type=\"file\"] {\n",
    " \n",
    "padding: 10px; border-radius: 5px;\n",
    "border: 1px solid #ccc; font-size: 16px;\n",
    "}\n",
    "\n",
    "\n",
    "button {\n",
    "padding: 12px; font-size: 16px;\n",
    "background-color: #5c6bc0; color: white;\n",
    "border: none; border-radius: 5px; cursor: pointer;\n",
    "transition: background-color 0.3s ease;\n",
    "}\n",
    "\n",
    "\n",
    "button:hover {\n",
    "background-color: #3f4a9a;\n",
    "}\n",
    "\n",
    "\n",
    "#emotionResult, #genreResult { margin-top: 20px;\n",
    "padding: 15px; background-color: #e1f5fe; border-radius: 5px;\n",
    " \n",
    "font-weight: bold; color: #0288d1; text-align: center;\n",
    "}\n",
    "\n",
    "\n",
    "#recordingControls { margin-top: 20px; text-align: center;\n",
    "}\n",
    "\n",
    "\n",
    "#audioPlayer { margin-top: 10px; display: none;\n",
    "}\n",
    "\n",
    "\n",
    "/* Responsive design */ @media (max-width: 600px) {\n",
    ".container { width: 95%; padding: 10px;\n",
    "}\n",
    "\n",
    "\n",
    "h1 {\n",
    "font-size: 24px;\n",
    "}\n",
    " \n",
    "h2 {\n",
    "font-size: 20px;\n",
    "}\n",
    "\n",
    "\n",
    "button {\n",
    "padding: 10px; font-size: 14px;\n",
    "}\n",
    "}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "<h1>Emotion and Genre Detection</h1>\n",
    "\n",
    "\n",
    "<div class=\"container\">\n",
    "<h2>Emotion Detection</h2>\n",
    "<form id=\"emotionForm\" enctype=\"multipart/form-data\">\n",
    "<input type=\"file\" name=\"audio\" accept=\"audio/*\" required>\n",
    "<button type=\"submit\">Predict Emotion</button>\n",
    "</form>\n",
    "<div id=\"recordingControls\">\n",
    "<button id=\"startRecording\">Start Recording</button>\n",
    "<button id=\"stopRecording\" style=\"display:none;\">Stop Recording</button>\n",
    "<audio id=\"audioPlayer\" controls style=\"display:none;\"></audio>\n",
    "</div>\n",
    " \n",
    "<div id=\"emotionResult\"></div>\n",
    "<h2>Genre Detection</h2>\n",
    "<form id=\"genreForm\" enctype=\"multipart/form-data\">\n",
    "<input type=\"file\" name=\"audio\" accept=\"audio/*\" required>\n",
    "<button type=\"submit\">Predict Genre</button>\n",
    "</form>\n",
    "<div id=\"genreResult\"></div> <!-- Genre result will be displayed here -->\n",
    "</div>\n",
    "\n",
    "\n",
    "<script>\n",
    "document.getElementById('emotionForm').addEventListener('submit', function(event) { event.preventDefault();\n",
    "const formData = new FormData(this);\n",
    "\n",
    "\n",
    "fetch('/predict_emotion', { method: 'POST',\n",
    "body: formData\n",
    "})\n",
    ".then(response => response.json())\n",
    ".then(data => {\n",
    "document.getElementById('emotionResult').innerHTML = 'Predicted Emotion: ' + data.emotion;\n",
    "})\n",
    ".catch(error => console.error('Error predicting emotion:', error));\n",
    "});\n",
    "\n",
    "\n",
    "document.getElementById('genreForm').addEventListener('submit', function(event) {\n",
    " \n",
    "event.preventDefault();\n",
    "const formData = new FormData(this);\n",
    "\n",
    "\n",
    "fetch('/predict_genre', { method: 'POST', body: formData\n",
    "})\n",
    ".then(response => response.json())\n",
    ".then(data => {\n",
    "document.getElementById('genreResult').innerHTML = 'Predicted Genre: ' + data.genre;\n",
    "})\n",
    ".catch(error => console.error('Error predicting genre:', error));\n",
    "});\n",
    "\n",
    "\n",
    "let audioChunks = []; let mediaRecorder;\n",
    "\n",
    "document.getElementById('startRecording').onclick = async () => {\n",
    "const stream = await navigator.mediaDevices.getUserMedia({ audio: true }); mediaRecorder = new MediaRecorder(stream);\n",
    "\n",
    "mediaRecorder.ondataavailable = (event) => { audioChunks.push(event.data);\n",
    "};\n",
    "\n",
    "\n",
    "mediaRecorder.onstop = () => {\n",
    " \n",
    "const audioBlob = new Blob(audioChunks, { type: 'audio/wav' }); const audioUrl = URL.createObjectURL(audioBlob);\n",
    "const audioPlayer = document.getElementById('audioPlayer'); audioPlayer.src = audioUrl;\n",
    "audioPlayer.style.display = 'block';\n",
    "\n",
    "\n",
    "let audioData = new FormData(); audioData.append('audio', audioBlob); fetch('/predict_emotion', {\n",
    "method: 'POST', body: audioData,\n",
    "})\n",
    ".then(response => response.json())\n",
    ".then(data => {\n",
    "document.getElementById('emotionResult').innerHTML = 'Predicted Emotion: ' + data.emotion;\n",
    "})\n",
    ".catch(error => {\n",
    "console.error(\"Error in prediction:\", error); document.getElementById('emotionResult').innerHTML = \"Error in prediction.\";\n",
    "});\n",
    "\n",
    "\n",
    "audioChunks = [];\n",
    "};\n",
    "\n",
    "mediaRecorder.start(); document.getElementById('startRecording').style.display = 'none';\n",
    " \n",
    "document.getElementById('stopRecording').style.display = 'inline';\n",
    "};\n",
    "\n",
    "\n",
    "// Stop Recording Button document.getElementById('stopRecording').onclick = () => {\n",
    "mediaRecorder.stop(); document.getElementById('stopRecording').style.display = 'none'; document.getElementById('startRecording').style.display = 'inline';\n",
    "};\n",
    "</script>\n",
    "</body>\n",
    "</html>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
